{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GroupShuffleSplit, GroupKFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =============================================================\n",
    "# BASELINE-VERSION (praktisch kein Preprocessing)\n",
    "# - Spaltenauswahl: nur 'Gaze X', 'Gaze Y'\n",
    "# - Typkonvertierung -> numerisch\n",
    "# - Fehlende Werte: nur lÃ¶schen (minimal-invasiv), sonst nichts\n",
    "# - Keine Outlier-Behandlung, keine Filter, kein Scaling\n",
    "# - Saubere Evaluation per Group-Splits (pro Datei)\n",
    "# - Drei Methoden: 80/20 GroupSplit, 5-Fold GroupKFold, OOB\n",
    "# =============================================================\n",
    "\n",
    "# ------------------------ Laden ------------------------ #\n",
    "def to_f32(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.astype(np.float32, copy=False)\n",
    "\n",
    "\n",
    "def load_csv(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, decimal=\",\", low_memory=False)\n",
    "    # robuste Spaltenwahl\n",
    "    cols_lc = {c.strip().lower(): c for c in df.columns}\n",
    "    col_x = cols_lc.get(\"gaze x\", \"Gaze X\")\n",
    "    col_y = cols_lc.get(\"gaze y\", \"Gaze Y\")\n",
    "    df = df[[col_x, col_y]].copy()\n",
    "    df = df.apply(pd.to_numeric, errors=\"coerce\")\n",
    "    # Minimal: fehlende Zeilen konsequent entfernen\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    return to_f32(df)\n",
    "\n",
    "\n",
    "def preload_folder(folder: str, target: int):\n",
    "    out = []\n",
    "    for fn in os.listdir(folder):\n",
    "        if fn.lower().endswith(\".csv\"):\n",
    "            p = os.path.join(folder, fn)\n",
    "            try:\n",
    "                d = load_csv(p)\n",
    "                if not d.empty:\n",
    "                    out.append((fn, d, target))\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Fehler beim Laden {p}: {e}\")\n",
    "    return out\n",
    "\n",
    "\n",
    "# ------------------------ Dataset bauen ------------------------ #\n",
    "# as_features=True: 1 Zeile pro Datei (sehr schnell, robust, kein Leakage)\n",
    "# as_features=False: zeilenbasiert (grÃ¶ÃŸer, aber weiterhin gruppensicher)\n",
    "\n",
    "def build_dataset(preloaded_files, as_features: bool = True):\n",
    "    X_list, y_list, groups = [], [], []\n",
    "\n",
    "    for (fn, df, target) in preloaded_files:\n",
    "        if as_features:\n",
    "            feats = {\n",
    "                \"gx_mean\": df.iloc[:, 0].mean(),\n",
    "                \"gx_std\": df.iloc[:, 0].std(ddof=1),\n",
    "                \"gy_mean\": df.iloc[:, 1].mean(),\n",
    "                \"gy_std\": df.iloc[:, 1].std(ddof=1),\n",
    "                \"gx_iqr\": (df.iloc[:, 0].quantile(0.75) - df.iloc[:, 0].quantile(0.25)),\n",
    "                \"gy_iqr\": (df.iloc[:, 1].quantile(0.75) - df.iloc[:, 1].quantile(0.25)),\n",
    "            }\n",
    "            X_sample = pd.DataFrame([feats], index=[0])\n",
    "            X_list.append(X_sample)\n",
    "            y_list.append(np.array([target], dtype=np.int8))\n",
    "            groups.append(fn)\n",
    "        else:\n",
    "            X_list.append(df)\n",
    "            y_list.append(np.full(len(df), target, dtype=np.int8))\n",
    "            groups.extend([fn] * len(df))\n",
    "\n",
    "    if not X_list:\n",
    "        return pd.DataFrame(), np.array([]), []\n",
    "\n",
    "    X = pd.concat(X_list, ignore_index=True)\n",
    "    y = np.concatenate(y_list)\n",
    "    groups = np.array(groups)\n",
    "    return to_f32(X), y.astype(np.int8), groups\n",
    "\n",
    "\n",
    "# ------------------------ Evaluation ------------------------ #\n",
    "def evaluate_three_methods(X: pd.DataFrame, y: np.ndarray, groups: np.ndarray, n_splits: int = 5):\n",
    "    rows = []\n",
    "\n",
    "    # RandomForest als solide Baseline; OOB aktiviert (Bootstrap-Ersatz)\n",
    "    base_clf = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_features=\"log2\",\n",
    "        min_samples_leaf=2,\n",
    "        min_samples_split=2,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        oob_score=True,\n",
    "        bootstrap=True,\n",
    "    )\n",
    "\n",
    "    # A) 80/20 GroupSplit\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "    for train_idx, test_idx in gss.split(X, y, groups):\n",
    "        clf = base_clf\n",
    "        clf.fit(X.iloc[train_idx], y[train_idx])\n",
    "        pred = clf.predict(X.iloc[test_idx])\n",
    "        acc = accuracy_score(y[test_idx], pred)\n",
    "        rows.append([\"Baseline\", \"80/20 GroupSplit\", float(acc), 0.0])\n",
    "\n",
    "    # B) GroupKFold\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    cv_scores = cross_val_score(base_clf, X, y, groups=groups, cv=gkf, n_jobs=-1)\n",
    "    rows.append([\"Baseline\", f\"{n_splits}-Fold GroupKFold\", float(cv_scores.mean()), float(cv_scores.std())])\n",
    "\n",
    "    # C) OOB-Score\n",
    "    clf_oob = base_clf\n",
    "    clf_oob.fit(X, y)\n",
    "    oob_acc = getattr(clf_oob, \"oob_score_\", np.nan)\n",
    "    rows.append([\"Baseline\", \"OOB (Bootstrap-Ersatz)\", float(oob_acc), 0.0])\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n",
    "# ------------------------ Main ------------------------ #\n",
    "def main(folder_cheat: str,\n",
    "         folder_no_cheat: str,\n",
    "         save_path: str = \"baseline_validation.xlsx\",\n",
    "         as_features: bool = True):\n",
    "\n",
    "    t0 = time.time()\n",
    "    print(\"â³ Lade Daten einmaligâ€¦\")\n",
    "    cheat_files = preload_folder(folder_cheat, 1)\n",
    "    no_cheat_files = preload_folder(folder_no_cheat, 0)\n",
    "    preloaded = cheat_files + no_cheat_files\n",
    "    print(f\"âž¡ï¸ Geladen: {len(preloaded)} Dateien (Schummeln: {len(cheat_files)}, Nicht: {len(no_cheat_files)})\")\n",
    "\n",
    "    X, y, groups = build_dataset(preloaded, as_features=as_features)\n",
    "\n",
    "    if X.empty:\n",
    "        raise RuntimeError(\"Keine Daten nach Baseline-Vorbereitung. PrÃ¼fe Spaltennamen und CSV-Inhalt.\")\n",
    "\n",
    "    rows = evaluate_three_methods(X, y, groups, n_splits=5)\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"Pipeline\", \"Validation Method\", \"Mean Accuracy\", \"Std Deviation\"]) \n",
    "    df.to_excel(save_path, index=False)\n",
    "    print(f\"âœ… Fertig in {((time.time()-t0)/60):.1f} min â†’ {save_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # >>> Pfade anpassen <<<\n",
    "    folder_cheat = r\"C:\\Users\\oxije\\Dropbox\\Dissertation\\Experimente\\1. Experiment\\Originaldaten_split\\Schummeln\"\n",
    "    folder_no_cheat = r\"C:\\Users\\oxije\\Dropbox\\Dissertation\\Experimente\\1. Experiment\\Originaldaten_split\\Nicht_Schummeln\"\n",
    "\n",
    "\n",
    "    # as_features=True = empfohlen (1 Zeile pro Datei). Wenn du zeilenbasiert testen willst, setze False.\n",
    "    main(folder_cheat, folder_no_cheat,\n",
    "         save_path=\"baseline_validation.xlsx\",\n",
    "         as_features=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of Validation Methods and Accuracy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import itertools\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import (\n",
    "    GroupShuffleSplit,\n",
    "    GroupKFold,\n",
    "    cross_val_score,\n",
    ")\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =============================================================\n",
    "#   ZIEL\n",
    "#   - 3 Validierungsvarianten vergleichen (80/20, K-Fold, \"Bootstrap\")\n",
    "#   - Deutlich schneller & sauberer (kein Data Leakage Ã¼ber Zeilen!)\n",
    "#   - Daten NUR EINMAL laden\n",
    "#   - OOB-Score als Bootstrap-Ersatz (schnell + statistisch sinnvoll)\n",
    "# =============================================================\n",
    "\n",
    "# ------------------------ Utilities ------------------------ #\n",
    "def to_f32(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.astype(np.float32, copy=False)\n",
    "\n",
    "\n",
    "def load_csv(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, decimal=\",\", low_memory=False)\n",
    "    # robustes Spalten-Mapping\n",
    "    candidates = {c.strip().lower(): c for c in df.columns}\n",
    "    cols = [candidates.get(\"gaze x\"), candidates.get(\"gaze y\")]\n",
    "    if any(c is None for c in cols):\n",
    "        # Fallback auf exakte Namen\n",
    "        cols = [\"Gaze X\", \"Gaze Y\"]\n",
    "    df = df[cols]\n",
    "    df = df.apply(pd.to_numeric, errors=\"coerce\")\n",
    "    return to_f32(df)\n",
    "\n",
    "\n",
    "def preload_folder(folder: str, target: int):\n",
    "    out = []\n",
    "    for fn in os.listdir(folder):\n",
    "        if fn.lower().endswith(\".csv\"):\n",
    "            p = os.path.join(folder, fn)\n",
    "            try:\n",
    "                df = load_csv(p)\n",
    "                if not df.empty:\n",
    "                    out.append((fn, df, target))\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Fehler beim Laden {p}: {e}\")\n",
    "    return out\n",
    "\n",
    "\n",
    "# ------------------------ Preprocessing ------------------------ #\n",
    "def handle_missing_values(df: pd.DataFrame, method: str) -> pd.DataFrame:\n",
    "    if method == \"mean\":\n",
    "        return df.fillna(df.mean(numeric_only=True))\n",
    "    elif method == \"locf\":\n",
    "        return df.fillna(method=\"ffill\")\n",
    "    elif method == \"delete\":\n",
    "        return df.dropna()\n",
    "    return df\n",
    "\n",
    "\n",
    "def handle_outliers(df: pd.DataFrame, method: str, z_thresh: float = 3.0) -> pd.DataFrame:\n",
    "    mu = df.mean(numeric_only=True)\n",
    "    sigma = df.std(numeric_only=True).replace(0, np.nan)\n",
    "    z = (df - mu) / sigma\n",
    "    mask = z.abs() > z_thresh\n",
    "\n",
    "    if method == \"mean\":\n",
    "        out = df.copy()\n",
    "        for c in df.columns:\n",
    "            if mask[c].any():\n",
    "                out.loc[mask[c], c] = mu[c]\n",
    "        return out\n",
    "    elif method == \"locf\":\n",
    "        out = df.copy()\n",
    "        for c in df.columns:\n",
    "            col = out[c]\n",
    "            col = col.mask(mask[c])\n",
    "            out[c] = col.ffill()\n",
    "        return out\n",
    "    elif method == \"delete\":\n",
    "        return df[~mask.any(axis=1)]\n",
    "    return df\n",
    "\n",
    "\n",
    "def apply_feature_limits(df: pd.DataFrame, limits: bool = True) -> pd.DataFrame:\n",
    "    if not limits:\n",
    "        return df\n",
    "    m = (df.iloc[:, 0].between(0, 1920)) & (df.iloc[:, 1].between(0, 1080))\n",
    "    return df[m]\n",
    "\n",
    "\n",
    "def smooth_moving_average(df: pd.DataFrame, on: bool, window: int = 5) -> pd.DataFrame:\n",
    "    if not on or window <= 1:\n",
    "        return df\n",
    "    kernel = np.ones(window, dtype=np.float32) / window\n",
    "    out = df.copy()\n",
    "    for c in df.columns:\n",
    "        out[c] = np.convolve(df[c].values, kernel, mode=\"same\")\n",
    "    return out\n",
    "\n",
    "\n",
    "def low_pass_filter(df: pd.DataFrame, on: bool, cutoff: float = 0.1, fs: float = 30.0) -> pd.DataFrame:\n",
    "    if not on:\n",
    "        return df\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = max(min(cutoff / nyq, 0.99), 1e-6)\n",
    "    b, a = butter(1, normal_cutoff, btype=\"low\", analog=False)\n",
    "    arr = filtfilt(b, a, df.values, axis=0)\n",
    "    return pd.DataFrame(arr, columns=df.columns, dtype=np.float32)\n",
    "\n",
    "\n",
    "def normalize(df: pd.DataFrame, method: str) -> pd.DataFrame:\n",
    "    if method == \"minmax\":\n",
    "        scaler = MinMaxScaler()\n",
    "    elif method == \"robust\":\n",
    "        scaler = RobustScaler()\n",
    "    elif method == \"zscore\":\n",
    "        scaler = StandardScaler()\n",
    "    else:\n",
    "        return df\n",
    "    df[df.columns] = scaler.fit_transform(df[df.columns])\n",
    "    return to_f32(df)\n",
    "\n",
    "\n",
    "# Eine Datei -> preprocess -> (Option A) Zeilen behalten  |  (Option B) Feature-Vektor pro Datei\n",
    "\n",
    "def preprocess_one(df: pd.DataFrame,\n",
    "                   missing_method, outlier_method, normalize_method,\n",
    "                   feature_limits, smoothing_on, lpf_on,\n",
    "                   as_features: bool):\n",
    "    d = df\n",
    "    d = handle_missing_values(d, missing_method) if missing_method else d\n",
    "    d = handle_outliers(d, outlier_method) if outlier_method else d\n",
    "    d = apply_feature_limits(d, feature_limits) if feature_limits is not None else d\n",
    "    d = smooth_moving_average(d, smoothing_on)\n",
    "    d = low_pass_filter(d, lpf_on)\n",
    "    d = normalize(d, normalize_method) if normalize_method else d\n",
    "\n",
    "    if as_features:\n",
    "        # Minimal-Feature-Set (extrem schnell). Kann spÃ¤ter erweitert werden (Fixationen etc.)\n",
    "        feats = {\n",
    "            \"gx_mean\": d.iloc[:, 0].mean(),\n",
    "            \"gx_std\": d.iloc[:, 0].std(ddof=1),\n",
    "            \"gy_mean\": d.iloc[:, 1].mean(),\n",
    "            \"gy_std\": d.iloc[:, 1].std(ddof=1),\n",
    "            \"gx_iqr\": (d.iloc[:, 0].quantile(0.75) - d.iloc[:, 0].quantile(0.25)),\n",
    "            \"gy_iqr\": (d.iloc[:, 1].quantile(0.75) - d.iloc[:, 1].quantile(0.25)),\n",
    "        }\n",
    "        return pd.DataFrame([feats])\n",
    "    else:\n",
    "        return d.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def build_dataset(preloaded_files,\n",
    "                  missing_method, outlier_method, normalize_method,\n",
    "                  feature_limits, smoothing_on, lpf_on,\n",
    "                  as_features: bool):\n",
    "    X_list, y_list, groups = [], [], []\n",
    "    for (fn, df, target) in preloaded_files:\n",
    "        d = preprocess_one(df, missing_method, outlier_method, normalize_method,\n",
    "                           feature_limits, smoothing_on, lpf_on, as_features)\n",
    "        if d.empty:\n",
    "            continue\n",
    "        X_list.append(d)\n",
    "        y_list.append(np.full(len(d), target, dtype=np.int8))\n",
    "        # group = Dateiname, damit Splits nie Zeilen derselben Datei in Train/Test mischen\n",
    "        groups.extend([fn] * len(d))\n",
    "\n",
    "    if not X_list:\n",
    "        return pd.DataFrame(), np.array([]), []\n",
    "\n",
    "    X = pd.concat(X_list, ignore_index=True)\n",
    "    y = np.concatenate(y_list)\n",
    "    return to_f32(X), y.astype(np.int8), np.array(groups)\n",
    "\n",
    "\n",
    "# ------------------------ Evaluation ------------------------ #\n",
    "@dataclass\n",
    "class EvalResult:\n",
    "    combo: tuple\n",
    "    method: str\n",
    "    mean_acc: float\n",
    "    std_acc: float\n",
    "\n",
    "\n",
    "def eval_all(X, y, groups, n_splits=5):\n",
    "    results = []\n",
    "\n",
    "    # Modell: parallel, OOB fÃ¼r \"Bootstrap\"-Ersatz\n",
    "    base_clf = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_features=\"log2\",\n",
    "        min_samples_leaf=2,\n",
    "        min_samples_split=2,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        oob_score=True,\n",
    "        bootstrap=True,\n",
    "    )\n",
    "\n",
    "    # A) 80/20 GroupSplit (kein Leakage)\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "    for train_idx, test_idx in gss.split(X, y, groups):\n",
    "        clf = base_clf\n",
    "        clf.fit(X.iloc[train_idx], y[train_idx])\n",
    "        pred = clf.predict(X.iloc[test_idx])\n",
    "        acc = accuracy_score(y[test_idx], pred)\n",
    "        results.append(EvalResult(\"\", \"80/20 GroupSplit\", float(acc), 0.0))\n",
    "\n",
    "    # B) GroupKFold CV\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    # cross_val_score clont intern das Estimator-Objekt; n_jobs=-1 parallelisiert die Folds\n",
    "    cv_scores = cross_val_score(base_clf, X, y, groups=groups, cv=gkf, n_jobs=-1)\n",
    "    results.append(EvalResult(\"\", f\"{n_splits}-Fold GroupKFold\", float(cv_scores.mean()), float(cv_scores.std())))\n",
    "\n",
    "    # C) \"Bootstrapping\": OOB-Accuracy (schnell, nahe .632-Bootstrap)\n",
    "    # Ein erneutes Fit auf ALLEN Daten, OOB-Samples dienen als Test\n",
    "    clf_oob = base_clf\n",
    "    clf_oob.fit(X, y)\n",
    "    oob_acc = getattr(clf_oob, \"oob_score_\", np.nan)\n",
    "    results.append(EvalResult(\"\", \"OOB (Bootstrap-Ersatz)\", float(oob_acc), 0.0))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# ------------------------ Hauptprogramm ------------------------ #\n",
    "def main(folder_cheat: str,\n",
    "         folder_no_cheat: str,\n",
    "         save_path: str = \"validation_comparison_optimized.xlsx\",\n",
    "         as_features: bool = True,  # True = extrem schnell & kein Leakage; False = zeilenbasiert\n",
    "         save_every: int = 20):\n",
    "\n",
    "    t0 = time.time()\n",
    "    print(\"â³ Lade Daten einmaligâ€¦\")\n",
    "    cheat_files = preload_folder(folder_cheat, 1)\n",
    "    no_cheat_files = preload_folder(folder_no_cheat, 0)\n",
    "    preloaded = cheat_files + no_cheat_files\n",
    "    print(f\"âž¡ï¸ Geladen: {len(preloaded)} Dateien (Schummeln: {len(cheat_files)}, Nicht: {len(no_cheat_files)})\")\n",
    "\n",
    "    # Grid\n",
    "    missing_methods = [\"mean\", \"locf\", \"delete\"]\n",
    "    outlier_methods = [\"mean\", \"locf\", \"delete\"]\n",
    "    normalize_methods = [\"minmax\", \"robust\", \"zscore\"]\n",
    "    feature_limits_options = [True, False]\n",
    "    smoothing_options = [True, False]\n",
    "    filter_options = [True, False]\n",
    "\n",
    "    combos = list(itertools.product(\n",
    "        missing_methods, outlier_methods, normalize_methods,\n",
    "        feature_limits_options, smoothing_options, filter_options\n",
    "    ))\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for i, combo in enumerate(combos, start=1):\n",
    "        (missing_method, outlier_method, normalize_method,\n",
    "         feature_limits, smoothing_on, lpf_on) = combo\n",
    "\n",
    "        X, y, groups = build_dataset(preloaded,\n",
    "                                     missing_method, outlier_method, normalize_method,\n",
    "                                     feature_limits, smoothing_on, lpf_on,\n",
    "                                     as_features=as_features)\n",
    "\n",
    "        if X.empty or np.isnan(X.values).any():\n",
    "            print(f\"âš ï¸ Skip {combo} (leer/NaN)\")\n",
    "            rows.append([combo, \"80/20 GroupSplit\", None, None])\n",
    "            rows.append([combo, \"5-Fold GroupKFold\", None, None])\n",
    "            rows.append([combo, \"OOB (Bootstrap-Ersatz)\", None, None])\n",
    "        else:\n",
    "            res = eval_all(X, y, groups, n_splits=5)\n",
    "            for r in res:\n",
    "                rows.append([combo, r.method, r.mean_acc, r.std_acc])\n",
    "\n",
    "        if i % save_every == 0:\n",
    "            df_partial = pd.DataFrame(rows, columns=[\"Preprocessing Combination\", \"Validation Method\", \"Mean Accuracy\", \"Std Deviation\"]) \n",
    "            tmp = save_path.replace(\".xlsx\", \"_partial.xlsx\")\n",
    "            df_partial.to_excel(tmp, index=False)\n",
    "            print(f\"ðŸ’¾ Zwischenspeicher nach {i}/{len(combos)} Kombos â†’ {tmp}\")\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"Preprocessing Combination\", \"Validation Method\", \"Mean Accuracy\", \"Std Deviation\"]) \n",
    "    df.to_excel(save_path, index=False)\n",
    "    print(f\"âœ… Fertig in {((time.time()-t0)/60):.1f} min â†’ {save_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # >>> Pfade anpassen <<<\n",
    "    folder_cheat = r\"C:\\\\Users\\\\oxije\\\\Dropbox\\\\Dissertation\\\\Experimente\\\\1. Experiment\\\\Originaldaten_split\\\\Schummeln\"\n",
    "    folder_no_cheat = r\"C:\\\\Users\\\\oxije\\\\Dropbox\\\\Dissertation\\\\Experimente\\\\1. Experiment\\\\Originaldaten_split\\\\Nicht_Schummeln\"\n",
    "\n",
    "    main(folder_cheat, folder_no_cheat,\n",
    "         save_path=\"validation_comparison_optimized.xlsx\",\n",
    "         as_features=True,   # True EMPFOHLEN (deutlich schneller + saubere Evaluation)\n",
    "         save_every=20)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
